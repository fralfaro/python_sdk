{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Property-based Testing: Hypothesis\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Cuando escribe pruebas unitarias, es difícil encontrar los casos de prueba correctos. Desea estar seguro de haber cubierto todos los casos interesantes, pero simplemente no puede conocer u olvidar uno de ellos. Por ejemplo, si realiza una prueba unitaria de una función que recibe un número entero, podría pensar en probar 0, 1 y 2. ¿Pero pensó en números negativos? ¿Qué pasa con los números grandes?\n",
    "\n",
    "**Property-based Testing** se refiere a la idea de escribir declaraciones que deberían ser verdaderas para su código (\"propiedades\"), y luego usar herramientas automatizadas para generar entradas de prueba (típicamente, entradas generadas al azar de un tipo apropiado), y observar si las propiedades son válidas para eso entrada. Si una entrada viola una propiedad, ha demostrado un error, así como un ejemplo conveniente que lo demuestra.\n",
    "\n",
    "\n",
    "En esta seccion se muetra un poco las estrategias de **Property-based Testing**  ocupando la librería [Hypothesis](https://hypothesis.readthedocs.io/en/latest/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: factorización de enteros\n",
    "\n",
    "Tenemos una función `factorize(n: int) -> List[int]` que toma un número entero y devuelve los factores primos:\n",
    "Un entero $n$ se llama número primo si es positivo y divisible exactamente por dos números: 1 y n.\n",
    "Queremos que el producto de los enteros devueltos sea el número en sí. Así es como diseñamos el comportamiento de las funciones:\n",
    "* factorize(0) = [0] - una excepción también habría sido razonable\n",
    "* factorize(1) = [1] - estrictamente hablando, 1 no es primo.\n",
    "* factorize(-1) = [-1] -… y ninguno es -1\n",
    "* factorize(-n) = [-1] + factorizar (n) para n> 1\n",
    "\n",
    "Una implementación podría verse así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting factorize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile factorize.py\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "\n",
    "def factorize(number: int) -> List[int]:\n",
    "    if number in [-1, 0, 1]:\n",
    "        return [number]\n",
    "    if number < 0:\n",
    "        return [-1] + factorize(-number)\n",
    "    factors = []\n",
    "\n",
    "    # Treat the factor 2 on its own\n",
    "    while number % 2 == 0:\n",
    "        factors.append(2)\n",
    "        number = number // 2\n",
    "    if number == 1:\n",
    "        return factors\n",
    "\n",
    "    # Now we only need to check uneven numbers\n",
    "    # up to the square root of the number\n",
    "    i = 3\n",
    "    while i <= int(math.ceil(number ** 0.5)) + 1:\n",
    "        while number % i == 0:\n",
    "            factors.append(i)\n",
    "            number = number // i\n",
    "        i += 2\n",
    "    return factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible que se sienta un poco incómodo por la condición en:\n",
    "```python\n",
    "while i <= int(math.ceil(number ** 0.5)) + 1:\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "entonces escribes una prueba para verificar los casos importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_factorize_parametrize.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_factorize_parametrize.py\n",
    "import pytest\n",
    "from factorize import factorize\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"n,expected\",\n",
    "    [\n",
    "        (0, [0]),  # 0\n",
    "        (1, [1]),  # 1\n",
    "        (-1, [-1]),  # -1\n",
    "        (-2, [-1, 2]),  # A prime, but negative\n",
    "        (2, [2]),  # Just one prime\n",
    "        (3, [3]),  # A different prime\n",
    "        (6, [2, 3]),  # Different primes\n",
    "        (8, [2, 2, 2]),  # Multiple times the same prime\n",
    "    ],\n",
    ")\n",
    "def test_factorize(n, expected):\n",
    "    assert factorize(n) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si la parametrización de la prueba no le resulta familiar, es posible que desee leer sobre `pytest.mark.parametrize`. Es increíble y esas pocas líneas ejecutan 8 pruebas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /home/falfaro/PycharmProjects/python_development_tools/python_development_tools/TDD\n",
      "plugins: cov-2.10.1, hypothesis-5.36.1\n",
      "collected 0 items / 1 error                                                    \u001b[0m\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m________________ ERROR collecting test_factorize_parametrize.py ________________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/home/falfaro/PycharmProjects/python_development_tools/python_development_tools/TDD/test_factorize_parametrize.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "test_factorize_parametrize.py:2: in <module>\n",
      "    from factorize import factorize\n",
      "E   ModuleNotFoundError: No module named 'factorize'\u001b[0m\n",
      "=========================== short test summary info ============================\n",
      "ERROR test_factorize_parametrize.py\n",
      "!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m=============================== \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.09s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_factorize_parametrize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cómo sería una prueba basada en propiedades para `factorize`?\n",
    "\n",
    "Primero, debemos pensar en la propiedad que queremos probar. Para `factorize` como lo diseñamos, sabemos que el producto de los números devueltos es igual al número en sí. Podemos introducir cualquier número entero, pero si los números enteros se vuelven demasiado grandes, el tiempo de ejecución será demasiado largo. Así que limitémoslos en un rango razonable de +/- un millón:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_factorize_property.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_factorize_property.py\n",
    "import hypothesis.strategies as st\n",
    "from hypothesis import given\n",
    "from factorize import factorize\n",
    "\n",
    "\n",
    "@given(st.integers(min_value=-(10 ** 6), max_value=10 ** 6))\n",
    "def test_factorize_multiplication_property(n):\n",
    "    \"\"\"The product of the integers returned by factorize(n) needs to be n.\"\"\"\n",
    "    factors = factorize(n)\n",
    "    product = 1\n",
    "    for factor in factors:\n",
    "        product *= factor\n",
    "    assert product == n, f\"factorize({n}) returned {factors}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ejecutar las pruebas desde la terminal (python -m pytest test_my_function.py), o si usa un IDE como Pycharm, especificando la configuración de pytest adecuada para su código.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /home/falfaro/PycharmProjects/python_development_tools/python_development_tools/TDD\n",
      "plugins: cov-2.10.1, hypothesis-5.36.1\n",
      "collected 0 items / 1 error                                                    \u001b[0m\n",
      "\n",
      "==================================== ERRORS ====================================\n",
      "\u001b[31m\u001b[1m_________________ ERROR collecting test_factorize_property.py __________________\u001b[0m\n",
      "\u001b[31mImportError while importing test module '/home/falfaro/PycharmProjects/python_development_tools/python_development_tools/TDD/test_factorize_property.py'.\n",
      "Hint: make sure your test modules/packages have valid Python names.\n",
      "Traceback:\n",
      "test_factorize_property.py:3: in <module>\n",
      "    from factorize import factorize\n",
      "E   ModuleNotFoundError: No module named 'factorize'\u001b[0m\n",
      "=========================== short test summary info ============================\n",
      "ERROR test_factorize_property.py\n",
      "!!!!!!!!!!!!!!!!!!!! Interrupted: 1 error during collection !!!!!!!!!!!!!!!!!!!!\n",
      "\u001b[31m=============================== \u001b[31m\u001b[1m1 error\u001b[0m\u001b[31m in 0.10s\u001b[0m\u001b[31m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest test_factorize_property.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede ver en el ejemplo anterior, la hipótesis descubrió que `factorize(5)` devolvió una lista vacía que no se multiplica por 5. Entonces podemos ver rápidamente que en realidad cometimos un error para todos los números primos: necesitamos sumar el número primo. Después de agregar la siguiente línea, las pruebas funcionan bien:\n",
    "\n",
    "```python\n",
    "if number != 1:\n",
    "    factors.append(number)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests más elaborados\n",
    "\n",
    "Este código es bastante sencillo: devolver un flotante a un tipo entero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing my_functions.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile my_functions.py\n",
    "def convert_to_integer(value: float) -> int:\n",
    "    return int(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoradores útiles\n",
    "\n",
    "Existen varias funcionalidades que nos ofrece `hypothesis`, dentro de las cuales se destacan:\n",
    "\n",
    "* `given`: convierte una función de prueba que acepta argumentos en una prueba aleatoria.\n",
    "* `example`: asegura que siempre se prueba un ejemplo específico.\n",
    "* `assume`: es como una afirmación que marca el ejemplo como malo, en lugar de fallar en la prueba.\n",
    "\n",
    "Veamos un ejemplo sencillo que mezcle estos tres atributos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_my_function.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_my_function.py\n",
    "import numpy as np\n",
    "import hypothesis.strategies as st\n",
    "from hypothesis import given,example, assume\n",
    "from my_functions import convert_to_integer\n",
    "\n",
    "\n",
    "@example(np.inf)\n",
    "@given(st.floats(allow_nan=False))\n",
    "def test_convert_to_integer(my_float):\n",
    "    assume(my_float!=np.inf)\n",
    "    assume(my_float!=-np.inf)\n",
    "\n",
    "    float_to_int = convert_to_integer(my_float)\n",
    "    assert isinstance(float_to_int, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos los test correspondientes asociados a este ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /home/falfaro/PycharmProjects/python_development_tools/python_development_tools/TDD\n",
      "plugins: cov-2.10.1, hypothesis-5.36.1\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_my_function.py \u001b[32m.\u001b[0m\u001b[32m                                                    [100%]\u001b[0m\n",
      "============================ Hypothesis Statistics =============================\n",
      "\n",
      "test_my_function.py::test_convert_to_integer:\n",
      "\n",
      "  - during reuse phase (0.00 seconds):\n",
      "    - Typical runtimes: < 1ms, ~ 43% in data generation\n",
      "    - 2 passing examples, 0 failing examples, 0 invalid examples\n",
      "\n",
      "  - during generate phase (0.14 seconds):\n",
      "    - Typical runtimes: 0-1 ms, ~ 42% in data generation\n",
      "    - 98 passing examples, 0 failing examples, 21 invalid examples\n",
      "\n",
      "  - Stopped because settings.max_examples=100\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.29s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_my_function.py --hypothesis-show-statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategias\n",
    "\n",
    "Una estrategia define los datos que genera Hypothesis para las pruebas y cómo se “simplifican” los ejemplos. En nuestro código, solo definimos los parámetros de los datos; la simplificación (o: \"shrinking\") es interna de Hypothesis.\n",
    "\n",
    "Comenzaremos con una estrategia que genera un valor flotante entre 0.0 y 10.0 (inclusive). Definimos esto en un archivo separado llamado `data_strategies.py`. Usar una clase de datos para esto puede parecer una exageración, pero es útil cuando está trabajando con un código más complejo que requiere un montón de parámetros diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data_strategies.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile data_strategies.py\n",
    "from pydantic import BaseModel\n",
    "import hypothesis.strategies as st\n",
    "\n",
    "\n",
    "class GeneratedData(BaseModel):\n",
    "    float_value: st.SearchStrategy[float]\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "\n",
    "\n",
    "generated_data = GeneratedData(float_value=st.floats(min_value=0.0, max_value=10.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez que hemos definido nuestra estrategia, agregamos un pequeño fragmento de código para pasar los ejemplos generados por la hipótesis a nuestra función y afirmar algo sobre el resultado requerido (la \"propiedad\") del código que queremos probar. El siguiente código extrae un valor flotante del objeto de clase de datos `generate_data` que definimos en el archivo `data_strategies.py` anterior, pasa ese valor a través de nuestra función `convert_to_integer` y finalmente afirma que se mantiene la propiedad esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_my_function.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_my_function.py\n",
    "import hypothesis.strategies as st\n",
    "from hypothesis import given\n",
    "from my_functions import convert_to_integer\n",
    "from data_strategies import generated_data\n",
    "\n",
    "\n",
    "@given(st.data())\n",
    "def test_convert_to_integer(\n",
    "        test_data: st.DataObject):\n",
    "\n",
    "    my_float = test_data.draw(generated_data.float_value)\n",
    "    float_to_int = convert_to_integer(my_float)\n",
    "\n",
    "    assert isinstance(float_to_int, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se corren las respectivas pruebas, agregando el comando extra `--hypothesis-show-statistics`, que muestra estadísticas relacionadas de las pruebas hechas por `Hypothesis`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /home/falfaro/PycharmProjects/python_development_tools/python_development_tools/TDD\n",
      "plugins: cov-2.10.1, hypothesis-5.36.1\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_my_function.py \u001b[32m.\u001b[0m\u001b[32m                                                    [100%]\u001b[0m\n",
      "============================ Hypothesis Statistics =============================\n",
      "\n",
      "test_my_function.py::test_convert_to_integer:\n",
      "\n",
      "  - during reuse phase (0.00 seconds):\n",
      "    - Typical runtimes: ~ 1ms, ~ 68% in data generation\n",
      "    - 1 passing examples, 0 failing examples, 0 invalid examples\n",
      "\n",
      "  - during generate phase (0.09 seconds):\n",
      "    - Typical runtimes: < 1ms, ~ 39% in data generation\n",
      "    - 99 passing examples, 0 failing examples, 0 invalid examples\n",
      "\n",
      "  - Stopped because settings.max_examples=100\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.15s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_my_function.py --hypothesis-show-statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuraciones\n",
    "Antes de ejecutar el módulo de prueba que desarrollamos anteriormente, revisemos algunas de las configuraciones que podemos usar para adaptar la hipótesis a nuestro caso de uso. La hipótesis viene con un montón de configuraciones. Estas configuraciones se pueden pasar a su función de prueba usando el decorador `settings()`, o registrando las configuraciones en un perfil y pasando el perfil usando el decorador (vea el código de ejemplo a continuación). \n",
    "\n",
    "Algunas configuraciones útiles incluyen:\n",
    "\n",
    "* `max_examples`: controla cuántos ejemplos de aprobación se requieren antes de que concluya la prueba. Esto es útil si tiene algunas pautas internas para el volumen de pruebas que se requieren para que un nuevo fragmento de código pase la revisión. Como regla general: cuanto más complejo sea el código, más ejemplos querrá ejecutar (los autores de Hypothesis señalan que lograron encontrar nuevos errores después de varios millones de ejemplos mientras probaban SymPy);\n",
    "\n",
    "* `deadline`: especifica cuánto tiempo puede tomar un ejemplo individual. Deberá aumentar este valor si tiene un código muy complejo en el que un ejemplo puede tardar más que el tiempo predeterminado en ejecutarse;\n",
    "\n",
    "* `suppress_health_check`: le permite especificar qué \"controles de salud\" ignorar. Útil cuando está trabajando con grandes conjuntos de datos (`HealthCheck.data_too_large`) o datos que tardan mucho en generarse (`HealthCheck.too_slow`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_my_function_with_settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_my_function_with_settings.py\n",
    "from hypothesis import given, settings, HealthCheck\n",
    "import hypothesis.strategies as st\n",
    "from my_functions import convert_to_integer\n",
    "from data_strategies import generated_data\n",
    "\n",
    "\n",
    "settings.register_profile(\n",
    "    \"my_profile\",\n",
    "    max_examples=200,\n",
    "    deadline=60 * 1000,  # Allow 1 min per example (deadline is specified in milliseconds)\n",
    "    suppress_health_check=(HealthCheck.too_slow, HealthCheck.data_too_large),\n",
    ")\n",
    "\n",
    "\n",
    "@given(st.data())\n",
    "@settings(settings.load_profile(\"my_profile\"))\n",
    "def test_convert_to_integer(\n",
    "        test_data: st.DataObject):\n",
    "\n",
    "    my_float = test_data.draw(generated_data.float_value)\n",
    "    float_to_int = convert_to_integer(my_float)\n",
    "\n",
    "    assert isinstance(float_to_int, int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corremos los respectivos tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.7.3, pytest-5.4.3, py-1.9.0, pluggy-0.13.1\n",
      "rootdir: /home/falfaro/PycharmProjects/python_development_tools/python_development_tools/TDD\n",
      "plugins: cov-2.10.1, hypothesis-5.36.1\n",
      "collected 1 item                                                               \u001b[0m\n",
      "\n",
      "test_my_function_with_settings.py \u001b[32m.\u001b[0m\u001b[32m                                      [100%]\u001b[0m\n",
      "============================ Hypothesis Statistics =============================\n",
      "\n",
      "test_my_function_with_settings.py::test_convert_to_integer:\n",
      "\n",
      "  - during reuse phase (0.00 seconds):\n",
      "    - Typical runtimes: < 1ms, ~ 57% in data generation\n",
      "    - 1 passing examples, 0 failing examples, 0 invalid examples\n",
      "\n",
      "  - during generate phase (0.12 seconds):\n",
      "    - Typical runtimes: < 1ms, ~ 39% in data generation\n",
      "    - 199 passing examples, 0 failing examples, 0 invalid examples\n",
      "\n",
      "  - Stopped because settings.max_examples=200\n",
      "\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.18s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python -m pytest test_my_function_with_settings.py --hypothesis-show-statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observación**: al final de cada presentación, se eliminan los archivos que generamos de manera temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar archivos temporales\n",
    "!rm -r *.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
