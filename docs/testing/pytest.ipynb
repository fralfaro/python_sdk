{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unit testing: Pytest\n",
    "\n",
    "## Introducción\n",
    "\n",
    "El framework [pytest](https://docs.pytest.org/en/stable/) facilita la escritura de pequeños tests, pero escala para admitir test funcionales complejas para aplicaciones y librerias.\n",
    "\n",
    "Si ha escrito pruebas unitarias para su código Python antes, es posible que haya utilizado el módulo `unittest` integrado de Python. `unittest` proporciona una base sólida sobre la cual construir su suite de pruebas, pero tiene algunas deficiencias.\n",
    "\n",
    "Varios marcos de prueba de terceros intentan abordar algunos de los problemas con unittest, y pytest ha demostrado ser uno de los más populares. pytest es un ecosistema basado en complementos y rico en funciones para probar su código Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Pytest\n",
    "Si no se especifican argumentos, los archivos de tests se buscan en ubicaciones desde las rutas de tests (si están configuradas) o el directorio actual. Alternativamente, los argumentos de la línea de comandos se pueden utilizar en cualquier combinación de directorios, nombres de archivos o ID de nodo.\n",
    "\n",
    "En los directorios seleccionados, pytest busca archivos de `test _ *.py` o `*_test.py`. En los archivos seleccionados, pytest busca funciones de test con prefijo test.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos un ejemplo sencillo de esto:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Escribir funciones a testear**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing algo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile algo.py\n",
    "def max(values):\n",
    "    _max = values[0]\n",
    "    for val in values:\n",
    "        if val > _max:\n",
    "            _max = val\n",
    "\n",
    "    return _max\n",
    "\n",
    "\n",
    "def min(values):\n",
    "    _min = values[0]\n",
    "\n",
    "    for val in values:\n",
    "        if val < _min:\n",
    "            _min = val\n",
    "\n",
    "    return _min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Escribir los tests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_min_max.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_min_max.py\n",
    "import algo\n",
    "\n",
    "def test_min():\n",
    "    values = (2, 3, 1, 4, 6)\n",
    "\n",
    "    val = algo.min(values)\n",
    "    assert val == 1\n",
    "\n",
    "def test_max():\n",
    "    values = (2, 3, 1, 4, 6)\n",
    "\n",
    "    val = algo.max(values)\n",
    "    assert val == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo `test_min_max.py` tiene una palabra de **test** en su nombre. Nos sirve para diferenciar entre un script de python genérico respecto a uno de testeo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\r\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\r\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 2 items                                                              \u001b[0m\r\n",
      "\r\n",
      "test_min_max.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                       [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest test_min_max.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la salida. Hubo dos pruebas y ambas han pasado con éxito. Se muestra una salida más detallada con `pytest -v test_min_max.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest skip\n",
    "\n",
    "Con el decorador de `skip`, podemos omitir los test especificados. Hay varias razones para saltarse el test; por ejemplo, una base de datos/servicio en línea no está disponible en este momento o nos saltamos los test específicas de Linux en Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing skipping.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile skipping.py\n",
    "import algo\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.skip\n",
    "def test_min():\n",
    "    values = (2, 3, 1, 4, 6)\n",
    "\n",
    "    val = algo.min(values)\n",
    "    assert val == 1\n",
    "\n",
    "def test_max():\n",
    "    values = (2, 3, 1, 4, 6)\n",
    "\n",
    "    val = algo.max(values)\n",
    "    assert val == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo, se omite `test_min()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\n",
      "collected 2 items                                                              \u001b[0m\n",
      "\n",
      "skipping.py \u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                           [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m========================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m1 skipped\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =========================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest skipping.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest Xfail\n",
    "\n",
    "Podemos usar el marcador `xfail` para indicar que espera que falle una prueba.\n",
    "\n",
    "Un caso de uso común para esto es cuando encuentra un error en su software y escribe una prueba para documentar cómo debería comportarse el software. Esta prueba, por supuesto, fallará hasta que corrija el error. \n",
    "\n",
    "Para evitar tener una prueba fallida, marca la prueba como `xfail`. Una vez que se corrige el error, elimina el marcador xfail y tiene una prueba de regresión que asegura que el error no se repita."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing xfail.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile xfail.py\n",
    "import pytest\n",
    "xfail = pytest.mark.xfail\n",
    "\n",
    "@xfail\n",
    "def test_hello():\n",
    "    assert 0\n",
    "\n",
    "@xfail(run=False)\n",
    "def test_hello2():\n",
    "    assert 0\n",
    "\n",
    "@xfail(\"hasattr(os, 'sep')\")\n",
    "def test_hello3():\n",
    "    assert 0\n",
    "\n",
    "@xfail(reason=\"bug 110\")\n",
    "def test_hello4():\n",
    "    assert 0\n",
    "\n",
    "@xfail('pytest.__version__[0] != \"17\"')\n",
    "def test_hello5():\n",
    "    assert 0\n",
    "\n",
    "def test_hello6():\n",
    "    pytest.xfail(\"reason\")\n",
    "\n",
    "@xfail(raises=IndexError)\n",
    "def test_hello7():\n",
    "    x = []\n",
    "    x[1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, todos los test serán ignorados (con una **x**), lo cuál no afectará al resto de los tests (suponiendo que los tests pasen correctamente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\n",
      "collected 7 items                                                              \u001b[0m\n",
      "\n",
      "xfail.py \u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33mx\u001b[0m\u001b[33m                                                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[33m============================== \u001b[33m\u001b[1m7 xfailed\u001b[0m\u001b[33m in 0.13s\u001b[0m\u001b[33m ==============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest xfail.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest marking\n",
    "Podemos usar marcadores para organizar los test unitarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing marking.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile marking.py\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.a\n",
    "def test_a1():\n",
    "\n",
    "    assert (1) == (1)\n",
    "\n",
    "@pytest.mark.a\n",
    "def test_a2():\n",
    "\n",
    "    assert (1, 2) == (1, 2)\n",
    "\n",
    "@pytest.mark.a\n",
    "def test_a3():\n",
    "\n",
    "    assert (1, 2, 3) == (1, 2, 3)\n",
    "\n",
    "@pytest.mark.b\n",
    "def test_b1():\n",
    "\n",
    "    assert \"falcon\" == \"fal\" + \"con\"\n",
    "\n",
    "@pytest.mark.b\n",
    "def test_b2():\n",
    "\n",
    "    assert \"falcon\" == f\"fal{'con'}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos dos grupos de tests identificados por marcadores, $a$ y $b$. Estas unidades son ejecutadas por `pytest -m a marking.py` y `pytest -m b marking.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\r\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\r\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 5 items / 2 deselected / 3 selected                                  \u001b[0m\r\n",
      "\r\n",
      "marking.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                           [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\r\n",
      "marking.py:3\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:3: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.a\r\n",
      "\r\n",
      "marking.py:8\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.a\r\n",
      "\r\n",
      "marking.py:13\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.a\r\n",
      "\r\n",
      "marking.py:18\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.b\r\n",
      "\r\n",
      "marking.py:23\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:23: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.b\r\n",
      "\r\n",
      "-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n",
      "\u001b[33m================= \u001b[32m3 passed\u001b[0m, \u001b[33m\u001b[1m2 deselected\u001b[0m, \u001b[33m\u001b[1m5 warnings\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[33m ==================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -m a marking.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\r\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\r\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 5 items / 3 deselected / 2 selected                                  \u001b[0m\r\n",
      "\r\n",
      "marking.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33m                                                            [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[33m=============================== warnings summary ===============================\u001b[0m\r\n",
      "marking.py:3\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:3: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.a\r\n",
      "\r\n",
      "marking.py:8\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:8: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.a\r\n",
      "\r\n",
      "marking.py:13\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:13: PytestUnknownMarkWarning: Unknown pytest.mark.a - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.a\r\n",
      "\r\n",
      "marking.py:18\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:18: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.b\r\n",
      "\r\n",
      "marking.py:23\r\n",
      "  /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing/marking.py:23: PytestUnknownMarkWarning: Unknown pytest.mark.b - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/latest/mark.html\r\n",
      "    @pytest.mark.b\r\n",
      "\r\n",
      "-- Docs: https://docs.pytest.org/en/latest/warnings.html\r\n",
      "\u001b[33m================= \u001b[32m2 passed\u001b[0m, \u001b[33m\u001b[1m3 deselected\u001b[0m, \u001b[33m\u001b[1m5 warnings\u001b[0m\u001b[33m in 0.02s\u001b[0m\u001b[33m ==================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest -m b marking.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest parameterized tests\n",
    "Con los tests parametrizados, podemos agregar múltiples valores a nuestras afirmaciones. Usamos el decorador `@pytest.mark.parametrize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing parameterized.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parameterized.py\n",
    "import algo\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\"data, expected\", [((2, 3, 1, 4, 6), 1), \n",
    "    ((5, -2, 0, 9, 12), -2), ((200, 100, 0, 300, 400), 0)])\n",
    "def test_min(data, expected):\n",
    "\n",
    "    val = algo.min(data)\n",
    "    assert val == expected\n",
    "\n",
    "@pytest.mark.parametrize(\"data, expected\", [((2, 3, 1, 4, 6), 6), \n",
    "    ((5, -2, 0, 9, 12), 12), ((200, 100, 0, 300, 400), 400)])\n",
    "def test_max(data, expected):\n",
    "\n",
    "    val = algo.max(data)\n",
    "    assert val == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasamos dos valores a la función de testeo: los datos y el valor esperado. En nuestro caso, probamos la función `min()` con tres tuplas de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\n",
      "collected 6 items                                                              \u001b[0m\n",
      "\n",
      "parameterized.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                                  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.03s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest parameterized.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest fixtures\n",
    "\n",
    "Los tests deben ejecutarse en el contexto de un conjunto conocido de objetos. Este conjunto de objetos se denomina `test fixture`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing algo2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile algo2.py\n",
    "def sel_sort(data):\n",
    "    if not isinstance(data, list):\n",
    "        vals = list(data)\n",
    "    else:\n",
    "        vals = data\n",
    "\n",
    "    size = len(vals)\n",
    "\n",
    "    for i in range(0, size):\n",
    "\n",
    "        for j in range(i+1, size):\n",
    "\n",
    "            if vals[j] < vals[i]:\n",
    "                _min = vals[j]\n",
    "                vals[j] = vals[i]\n",
    "                vals[i] = _min\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing fixtures.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile fixtures.py\n",
    "\n",
    "import algo2\n",
    "import pytest\n",
    "\n",
    "# fijar valor data\n",
    "@pytest.fixture\n",
    "def data():\n",
    "    return [3, 2, 1, 5, -3, 2, 0, -2, 11, 9]\n",
    "\n",
    "# pasar data como argumento del test\n",
    "def test_sel_sort(data):\n",
    "    sorted_vals = algo2.sel_sort(data)\n",
    "    assert sorted_vals == sorted(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\r\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\r\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 1 item                                                               \u001b[0m\r\n",
      "\r\n",
      "fixtures.py \u001b[32m.\u001b[0m\u001b[32m                                                            [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest fixtures.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observación**: para realizar los siguientes ejemplos, es necesario eliminar todos los archivos `.py` previos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar archivos .py\n",
    "!rm *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytest layouts\n",
    "Los tests de Python se pueden organizar de varias formas. Los tests se pueden integrar en el paquete de Python o pueden descansar fuera de la librería.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrated tests\n",
    "\n",
    "A continuación, mostramos cómo ejecutar pruebas dentro de un paquete de Python.\n",
    "\n",
    "```terminal\n",
    "setup.py\n",
    "utils\n",
    "│   algo.py\n",
    "│   srel.py\n",
    "│   __init__.py\n",
    "│\n",
    "└───tests\n",
    "        algo_test.py\n",
    "        srel_test.py\n",
    "        __init__.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tenemos este diseño del paquete. Los test se encuentran en el subdirectorio de **tests** dentro del paquete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) crear archivos previos del ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear carpetas\n",
    "!mkdir utils tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregar archivo __init__.py\n",
    "!touch utils/__init__.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregar archivo __init__.py\n",
    "!touch tests/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) crear archivos .py a testear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile setup.py\n",
    "from setuptools import setup, find_packages\n",
    "setup(name=\"utils\", packages=find_packages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/algo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/algo.py\n",
    "def sel_sort(data):\n",
    "    if not isinstance(data, list):\n",
    "        vals = list(data)\n",
    "    else:\n",
    "        vals = data\n",
    "\n",
    "    size = len(vals)\n",
    "\n",
    "    for i in range(0, size):\n",
    "\n",
    "        for j in range(i+1, size):\n",
    "\n",
    "            if vals[j] < vals[i]:\n",
    "                _min = vals[j]\n",
    "                vals[j] = vals[i]\n",
    "                vals[i] = _min\n",
    "    return vals\n",
    "\n",
    "def max(values):\n",
    "    _max = values[0]\n",
    "    for val in values:\n",
    "        if val > _max:\n",
    "            _max = val\n",
    "\n",
    "    return _max\n",
    "\n",
    "\n",
    "def min(values):\n",
    "    _min = values[0]\n",
    "\n",
    "    for val in values:\n",
    "        if val < _min:\n",
    "            _min = val\n",
    "\n",
    "    return _min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils/srel.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils/srel.py\n",
    "\n",
    "def is_palindrome(val):\n",
    "    return val == val[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) crear los tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/algo_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/algo_test.py\n",
    "import utils.algo\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"data\",\n",
    "    [\n",
    "        [3, 2, 1, 5, -3, 2, 0, -2, 11, 9],\n",
    "        (3, 2, 1, 5, -3, 2, 0, -2, 11, 9)\n",
    "    ]\n",
    ")\n",
    "def test_sel_sort(data):\n",
    "    sorted_vals = utils.algo.sel_sort(data)\n",
    "    assert sorted_vals == sorted(data)\n",
    "\n",
    "@pytest.fixture\n",
    "def values():\n",
    "    return (2, 3, 1, 4, 6)\n",
    "\n",
    "def test_min(values):\n",
    "    val = utils.algo.min(values)\n",
    "    assert val == 1\n",
    "\n",
    "def test_max(values):\n",
    "    val = utils.algo.max(values)\n",
    "    assert val == 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tests/srel_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/srel_test.py\n",
    "import utils.srel\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"word, expected\", \n",
    "    [\n",
    "        ('kayak', True), \n",
    "        ('civic', True), \n",
    "        ('forest', False)\n",
    "    ]\n",
    ")\n",
    "def test_palindrome(word, expected):\n",
    "\n",
    "    val = utils.srel.is_palindrome(word)\n",
    "    assert val == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dejar tests en la ruta correcta\n",
    "!mv tests utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) correr los test !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\n",
      "collected 7 items                                                              \u001b[0m\n",
      "\n",
      "utils/tests/algo_test.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                            [ 57%]\u001b[0m\n",
      "utils/tests/srel_test.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                             [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pytest --pyargs utils "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests outside the package\n",
    "El siguiente ejemplo muestra un diseño dela aplicación donde los tests no están integrados dentro del paquete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```terminal\n",
    "setup.py\n",
    "src\n",
    "└───utils\n",
    "│       algo.py\n",
    "│       srel.py\n",
    "tests\n",
    "    algo_test.py\n",
    "    srel_test.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este diseño, tenemos tets fuera del árbol de fuentes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) crear diseño del ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crear carpeta src\n",
    "!mkdir src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mover carpeta utils\n",
    "!mv utils  src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mover carpeta tests\n",
    "!mv src/utils/tests  src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Correr los test !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\r\n",
      "platform linux -- Python 3.8.5, pytest-5.4.3, py-1.10.0, pluggy-0.13.1\r\n",
      "rootdir: /home/fralfaro/PycharmProjects/python_development_tools/python_development_tools/testing\r\n",
      "plugins: hypothesis-5.49.0, cov-2.11.1\r\n",
      "\u001b[1mcollecting ... \u001b[0m\u001b[1m\r",
      "collected 7 items                                                              \u001b[0m\r\n",
      "\r\n",
      "src/tests/algo_test.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                              [ 57%]\u001b[0m\r\n",
      "src/tests/srel_test.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                               [100%]\u001b[0m\r\n",
      "\r\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m7 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observación**: al final de cada presentación, se eliminan los archivos que generamos de manera temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminar archivos temporales\n",
    "#!rm -r cov_annotate cov_html htmlcov src *.py *.xml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
